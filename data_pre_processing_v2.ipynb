{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVrENMp+2glgghHKk6Xarf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amaluvincent/Fake-News-Detection/blob/main/data_pre_processing_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4C0r9H9cmFm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1B7-i_rLRy_"
      },
      "source": [
        "# 1.Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pRn0hqa3yxUJ",
        "outputId": "bac8e6c1-b66e-4c82-8296-572e64d57623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib seaborn scikit-learn nltk\n",
        "!pip install tensorflow keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV7-QWloLZtp"
      },
      "source": [
        "# 2.Importing libraraies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWyNOScPLdpd",
        "outputId": "ea1bd910-831d-432a-e806-bff9f672078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# import necessary libabaries\n",
        "\n",
        "import pandas as pd # For data manipulation and handling structured datasets.\n",
        "import matplotlib.pyplot as plt  # For creating visualizations like plots and charts.\n",
        "import seaborn as sns  # For advanced and aesthetically pleasing visualizations.\n",
        "import numpy as np  # For numerical computations and handling arrays/matrices.\n",
        "from sklearn.pipeline import Pipeline  # For creating a pipeline of machine learning models.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # For combining tokenization, TF-IDF transformation, and vectorization .\n",
        "from sklearn import feature_extraction, linear_model, model_selection # For feature engineering,linear model and model selection.\n",
        "from sklearn.model_selection import train_test_split #For splitting data into test and train sets.\n",
        "from sklearn import metrics # For evaluating the performance of machine learning models.\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix, classification_report  # For evaluating the performance.\n",
        "from sklearn.model_selection import cross_val_score # For handling cross validation.\n",
        "from sklearn.model_selection import GridSearchCV # For evaluating models\n",
        "from sklearn.linear_model import LogisticRegression # For importing Logistic Regression model.\n",
        "from sklearn.ensemble import RandomForestClassifier # FOr importing Random forest model.\n",
        "from sklearn.tree import DecisionTreeClassifier # For importing Decision tree model.\n",
        "from sklearn.naive_bayes import MultinomialNB  # FOr importing Naive Bayes model.\n",
        "import string # For handling string operations relevant to text preprocessing.\n",
        "\n",
        "import nltk  # For working with human language data(text).\n",
        "nltk.download('stopwords') # Downloads a list of comomn words (\"like\",\"the\",\"a\",\"is\") called stopwords.\n",
        "nltk.download('punkt_tab') # Downloads the 'punkt' resource, which is used for tokenization .\n",
        "nltk.download('wordnet') #For tasks like finding synonyms, antonyms, and understanding the relationships between words.\n",
        "from nltk.corpus import stopwords  # For accessing the list of stopwords.\n",
        "from nltk.stem import WordNetLemmatizer  # For reducing words to their base form.\n",
        "from nltk.tokenize import word_tokenize # FOr splitting text into individual words.\n",
        "from wordcloud import WordCloud  # FOr creating visual representation of word frequencies.\n",
        "from collections import Counter # For counting the frequency of items .\n",
        "import warnings # Importing warning module.\n",
        "warnings.filterwarnings(\"ignore\") #FOr ignoring warning messages.\n",
        "\n",
        "from tensorflow.keras.models import Sequential  # For creating a linear stack of layers for LSTM\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional # help the model learn patterns in the text.\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # breaking text intoo words\n",
        "from tensorflow.keras.callbacks import EarlyStopping # tool for preventing overfitting\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #To maintain uniform length\n",
        "from sklearn.model_selection import KFold #  For splitting data into training and validation sets for k-fold cross-validation.\n",
        "from tensorflow.keras.regularizers import l2 # To prevent overfitting by adding penalties to the model's complexity.\n",
        "from sklearn.utils.class_weight import compute_class_weight # handle  imbalanced dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBTwx1_6MAXs"
      },
      "source": [
        "# 3.Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jgU4AHdmuWp",
        "outputId": "6cee8e49-2d57-4828-9be0-3f3822bb1002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true: (21417, 4)\n",
            "fake: (23481, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load the ISOT dataset\n",
        "true_news = pd.read_csv('/content/True.csv')\n",
        "fake_news =  pd.read_csv('/content/Fake.csv')\n",
        "\n",
        "# checking the size of each files\n",
        "print('true:',true_news.shape)\n",
        "print('fake:', fake_news.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXErUxH0dTpX",
        "outputId": "5ccca93c-ca8f-4640-cf49-b69fd89fb77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True News Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21417 entries, 0 to 21416\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    21417 non-null  object\n",
            " 1   text     21417 non-null  object\n",
            " 2   subject  21417 non-null  object\n",
            " 3   date     21417 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 669.4+ KB\n",
            "\n",
            "Fake News Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23481 entries, 0 to 23480\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    23481 non-null  object\n",
            " 1   text     23481 non-null  object\n",
            " 2   subject  23481 non-null  object\n",
            " 3   date     23481 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 733.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# Display information about the true news dataset\n",
        "print(\"True News Dataset Info:\")\n",
        "true_news.info()\n",
        "\n",
        "# Display information about the fake news dataset\n",
        "print(\"\\nFake News Dataset Info:\")\n",
        "fake_news.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epLz_IgBmvtE"
      },
      "source": [
        "# 4. Data cleaning and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cGxredleaCW",
        "outputId": "831c212a-4a93-444f-9c98-48c765000e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values in True News dataset:\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n",
            "\n",
            "Null values in Fake News dataset:\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in each column of true_news and fake_news\n",
        "print(\"Null values in True News dataset:\")\n",
        "print(true_news.isnull().sum())\n",
        "\n",
        "print(\"\\nNull values in Fake News dataset:\")\n",
        "print(fake_news.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9msG6vo4MCi5",
        "outputId": "219faae9-7ada-4ea0-ff88-249eb27bbaf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0  FLASHBACK: ‘The Syrian War: What You’re Not Be...   \n",
            "1  Trump administration defends travel ban in Sup...   \n",
            "2   Lindsey Graham Is Getting All War Tingly Agai...   \n",
            "3  Trump pulls nearly even with Clinton after Rep...   \n",
            "4   The Internet Can’t Stop Laughing After Sean S...   \n",
            "\n",
            "                                                text       subject  \\\n",
            "0  21st Century Wire says Back in August 2013, Un...   Middle-east   \n",
            "1  NEW YORK (Reuters) - President Donald Trump’s ...  politicsNews   \n",
            "2  Every once in a while, it seems like Sen. Lind...          News   \n",
            "3   NEW YORK (Reuters) - Republican presidential ...  politicsNews   \n",
            "4  White House Press Secretary Sean Spicer told r...          News   \n",
            "\n",
            "                date  label  \n",
            "0  December 18, 2016      1  \n",
            "1   August 11, 2017       0  \n",
            "2     August 1, 2017      1  \n",
            "3     July 22, 2016       0  \n",
            "4  February 14, 2017      1  \n",
            "(44898, 5)\n"
          ]
        }
      ],
      "source": [
        "# Add a label to each dataframe\n",
        "true_news['label'] = 0   # 0 for legitimate news\n",
        "fake_news['label'] = 1   # 1 for fake news\n",
        "\n",
        "# Combine both datasets\n",
        "df = pd.concat([true_news, fake_news],ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)  # shuffle the data\n",
        "\n",
        "# Print the combined dataset (first few rows)\n",
        "print(df.head(5))\n",
        "print(df.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n71hxbHxqL1A",
        "outputId": "fe27dbcc-7b82-4fd3-d6c3-9ce67f13e65e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in dataset:\n",
            " title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "label      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values after combined\n",
        "print(\"Missing values in dataset:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iUWYe0erMKk",
        "outputId": "edb42c69-2fef-482b-8210-a96c51d8565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44898 entries, 0 to 44897\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    44898 non-null  object\n",
            " 1   text     44898 non-null  object\n",
            " 2   subject  44898 non-null  object\n",
            " 3   date     44898 non-null  object\n",
            " 4   label    44898 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 1.7+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# print dataset info\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzVZy5UZtKZP",
        "outputId": "a622f157-f99d-4610-ac19-1b1520155b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text       subject  label\n",
            "0  21st Century Wire says Back in August 2013, Un...   Middle-east      1\n",
            "1  NEW YORK (Reuters) - President Donald Trump’s ...  politicsNews      0\n",
            "2  Every once in a while, it seems like Sen. Lind...          News      1\n",
            "3   NEW YORK (Reuters) - Republican presidential ...  politicsNews      0\n",
            "4  White House Press Secretary Sean Spicer told r...          News      1\n"
          ]
        }
      ],
      "source": [
        "# Removing unnecessary column(date)and (title)\n",
        "df = df.drop(columns=[\"date\", \"title\"])\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRQdnIocYL8b",
        "outputId": "b157b164-a195-474e-9314-b26fc38d76e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text       subject  label\n",
            "0  21st Century Wire says Back in August 2013, Un...   Middle-east      1\n",
            "1  NEW YORK () - President Donald Trump’s adminis...  politicsNews      0\n",
            "2  Every once in a while, it seems like Sen. Lind...          News      1\n",
            "3   NEW YORK () - Republican presidential nominee...  politicsNews      0\n",
            "4  White House Press Secretary Sean Spicer told r...          News      1\n"
          ]
        }
      ],
      "source": [
        "# Removing the word \"Reuters\"(which is the site were true news collected) in the text\n",
        "df['text'] = df['text'].str.replace('Reuters', '', case=False)\n",
        "print(df.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjmdbM4kUuhB",
        "outputId": "f3668478-5728-45df-9a49-dd15e182d3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before cleaning:\n",
            "Can you believe it? The clown that can t keep his hands off of women and the faux Indian socialist could be pairing up for a White House run. We really are in the age of the low information voter when these two are seriously considering running and people are seriously considering voting for them. NUTS! Joe Biden has been making his 2016 deliberations all about his late son since August.Aug. 1, to be exact   the day renowned Hillary Clinton-critic Maureen Dowd published a column that marked a turning point in the presidential speculation.BIDEN GAFFES: Read more: POLITICO\n",
            "\n",
            "After cleaning:\n",
            "Can you believe it? The clown that can t keep his hands off of women and the faux Indian socialist could be pairing up for a White House run. We really are in the age of the low information voter when these two are seriously considering running and people are seriously considering voting for them. NUTS! Joe Biden has been making his 2016 deliberations all about his late son since August.Aug. 1, to be exact   the day renowned Hillary Clinton-critic Maureen Dowd published a column that marked a turning point in the presidential speculation.BIDEN GAFFES: Read more: POLITICO\n"
          ]
        }
      ],
      "source": [
        "# Removing the word \"Getty images\" (which is a visual media company)& \"Featured image\"in the text\n",
        "# eg:Print the text before cleaning for the 28th row\n",
        "print(\"Before cleaning:\")\n",
        "print(df['text'].iloc[28])\n",
        "\n",
        "# Perform the cleaning\n",
        "df['text'] = df['text'].str.replace('Getty Images', '', case=False)\n",
        "df['text'] = df['text'].str.replace('Featured image', '', case=False)\n",
        "\n",
        "# Print the text after cleaning for the 28th row\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(df['text'].iloc[28])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjZODb2fUaG2",
        "outputId": "e00de630-eab1-4043-80a4-f7bf34d8de0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Changes across all texts:\n",
            " Lowercasing Change (%)       94.690187\n",
            "Non-Alpha Removed (%)        13.402350\n",
            "Stopwords Removed (%)        37.077629\n",
            "Lemmatization Changes (%)     5.621640\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Define stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to calculate changes per preprocessing step\n",
        "def analyze_preprocessing(text):\n",
        "    if pd.isnull(text) or text.strip() == \"\":  # Handle NaN or empty string input\n",
        "        return {\n",
        "            'Lowercasing Change (%)': 0,\n",
        "            'Non-Alpha Removed (%)': 0,\n",
        "            'Stopwords Removed (%)': 0,\n",
        "            'Lemmatization Changes (%)': 0,\n",
        "            'Cleaned Text': ''\n",
        "        }\n",
        "    # Initial count of tokens\n",
        "    original_tokens = word_tokenize(text)   # Tokenization\n",
        "    total_tokens = len(original_tokens)\n",
        "\n",
        "     # Check if there are no tokens, return 0 for all changes\n",
        "    if total_tokens == 0:\n",
        "        return {\n",
        "            'Lowercasing Change (%)': 0,\n",
        "            'Non-Alpha Removed (%)': 0,\n",
        "            'Stopwords Removed (%)': 0,\n",
        "            'Lemmatization Changes (%)': 0,\n",
        "        }\n",
        "\n",
        "    # Step-by-step preprocessing\n",
        "    lowercased_text = text.lower()    # Lowercasing\n",
        "    lowercased_tokens = word_tokenize(lowercased_text)    #Tokenization\n",
        "    alphabetic_tokens = [word for word in lowercased_tokens if word.isalpha()]  #Remove non-alphabetic tokens\n",
        "    non_stopword_tokens = [word for word in alphabetic_tokens if word not in stop_words]  # Stopword removal\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in non_stopword_tokens]    # Lemmatization\n",
        "\n",
        "    # Calculate changes as percentages\n",
        "    #total_tokens = len(original_tokens)\n",
        "    lowercasing_change = 100 if any(word.isupper() for word in original_tokens) else 0\n",
        "    non_alpha_removed = (len(lowercased_tokens) - len(alphabetic_tokens)) / total_tokens * 100 if total_tokens > 0 else 0\n",
        "    stopword_removed = (len(alphabetic_tokens) - len(non_stopword_tokens)) / total_tokens * 100 if total_tokens > 0 else 0\n",
        "    lemmatization_changes = sum(1 for orig, lemma in zip(non_stopword_tokens, lemmatized_tokens) if orig != lemma) / total_tokens * 100 if non_stopword_tokens else 0\n",
        "\n",
        "     # Join the lemmatized tokens to form the cleaned text\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return {\n",
        "        'Lowercasing Change (%)': lowercasing_change,\n",
        "        'Non-Alpha Removed (%)': non_alpha_removed,\n",
        "        'Stopwords Removed (%)': stopword_removed,\n",
        "        'Lemmatization Changes (%)': lemmatization_changes,\n",
        "        'Cleaned Text': cleaned_text\n",
        "    }\n",
        "\n",
        "# Apply the analysis function to each text in the dataframe and create a new dataframe to store results\n",
        "df[['Lowercasing Change (%)', 'Non-Alpha Removed (%)', 'Stopwords Removed (%)',\n",
        "    'Lemmatization Changes (%)','cleaned_text']] = df['text'].apply(lambda x: pd.Series(analyze_preprocessing(x)))\n",
        "\n",
        "# Display average percentages across all text entries\n",
        "average_changes = df[['Lowercasing Change (%)', 'Non-Alpha Removed (%)',\n",
        "                      'Stopwords Removed (%)', 'Lemmatization Changes (%)']].mean()\n",
        "print(\"Average Changes across all texts:\\n\", average_changes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzchM-iD9pDZ",
        "outputId": "dec123f3-96fc-4ca7-e803-ccee500de563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  21st Century Wire says Back in August 2013, Un...   \n",
            "1  NEW YORK () - President Donald Trump’s adminis...   \n",
            "2  Every once in a while, it seems like Sen. Lind...   \n",
            "3   NEW YORK () - Republican presidential nominee...   \n",
            "4  White House Press Secretary Sean Spicer told r...   \n",
            "5  Former Vice President Joe Biden was asked on M...   \n",
            "6  SEOUL () - South Korean President Moon Jae-in ...   \n",
            "7  CAIRO () - An Egyptian court sentenced a Briti...   \n",
            "8  Listen:After creating a video that included a ...   \n",
            "9  What a role model for women and young girls, a...   \n",
            "\n",
            "                                        cleaned_text  Lowercasing Change (%)  \\\n",
            "0  century wire say back august united nation inv...                     100   \n",
            "1  new york president donald trump administration...                     100   \n",
            "2  every seems like lindsey graham might growing ...                     100   \n",
            "3  new york republican presidential nominee donal...                     100   \n",
            "4  white house press secretary sean spicer told r...                     100   \n",
            "5  former vice president joe biden asked monday m...                     100   \n",
            "6  seoul south korean president moon initially su...                     100   \n",
            "7  cairo egyptian court sentenced british woman t...                     100   \n",
            "8  listen creating video included vile beheaded b...                     100   \n",
            "9  role model woman young girl presidential candi...                     100   \n",
            "\n",
            "   Non-Alpha Removed (%)  Stopwords Removed (%)  Lemmatization Changes (%)  \n",
            "0              13.437500              33.750000                   6.250000  \n",
            "1              13.914657              35.807050                   7.792208  \n",
            "2              13.983051              44.067797                   2.542373  \n",
            "3              14.588859              36.339523                   4.509284  \n",
            "4              23.779193              30.997877                   3.609342  \n",
            "5              16.144578              40.963855                   3.855422  \n",
            "6              15.463918              30.927835                   2.061856  \n",
            "7              12.832930              41.162228                   3.147700  \n",
            "8              12.500000              39.583333                   2.976190  \n",
            "9              15.771231              34.662045                   5.199307  \n"
          ]
        }
      ],
      "source": [
        "# Print the original and cleaned text\n",
        "print(df[['text','cleaned_text','Lowercasing Change (%)', 'Non-Alpha Removed (%)',\n",
        "          'Stopwords Removed (%)', 'Lemmatization Changes (%)']].head(10))  # Display the first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0X5rfWAuOgl",
        "outputId": "8f3a6ac3-66b0-4970-cb6e-38fc819f6cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows in X: 6468\n",
            "Number of duplicate rows in combined X and y: 6467\n"
          ]
        }
      ],
      "source": [
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "\n",
        "# Check for duplicates in the input features\n",
        "print(f\"Number of duplicate rows in X: {X.duplicated().sum()}\")\n",
        "\n",
        "# Check for duplicates in the combination of X and y (to consider label alignment)\n",
        "data = pd.DataFrame({'X': X, 'y': y})\n",
        "print(f\"Number of duplicate rows in combined X and y: {data.duplicated().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKdp0kRswxHP",
        "outputId": "bb0b001c-4af2-4092-8c08-4ae675b054da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows in X after cleaning: 1\n",
            "Number of duplicate rows in combined X and y after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Combine X and y into a single DataFrame\n",
        "data = pd.DataFrame({'X': X, 'y': y})\n",
        "\n",
        "# Drop duplicates based on both features and labels\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Separate the cleaned data back into X and y\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "# Print the number of duplicates after cleaning\n",
        "print(f\"Number of duplicate rows in X after cleaning: {X.duplicated().sum()}\")\n",
        "print(f\"Number of duplicate rows in combined X and y after cleaning: {data.duplicated().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzV1iyYkGoUj",
        "outputId": "5e5db520-9aa8-44ed-8f77-028716a73b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in 'cleaned_text': 0\n"
          ]
        }
      ],
      "source": [
        "# Check for NaN values in 'cleaned_text' after preprocessing\n",
        "missing_values = df['cleaned_text'].isna().sum()\n",
        "print(f\"Number of NaN values in 'cleaned_text': {missing_values}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNW9vfcE0EfN"
      },
      "source": [
        "# 5.Exploratory Data Analysis"
      ]
    }
  ]
}